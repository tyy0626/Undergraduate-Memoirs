{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa10043f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing Libraries and Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import KFold\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b297f31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download stopwords，quiet=True means hiding the download information\n",
    "nltk.download('stopwords', quiet=True)\n",
    "# set up stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# Initialize the Porter stemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9cb253ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngrams(words, n):\n",
    "    \"\"\"\n",
    "    Generate n-gram sequences\n",
    "    :param words: word list\n",
    "    :param n: The value of n for n-grams\n",
    "    :return: generate n-gram list\n",
    "    \"\"\"\n",
    "    return [' '.join(words[i:i+n]) for i in range(len(words) - n + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c1f91f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_with_trigrams(text):\n",
    "    \"\"\"\n",
    "    Text preprocessing functions, including conversion to lowercase, punctuation removal, stop word filtering, stemming, and generating unigrams, bigrams, and trigrams\n",
    "    :param text: Original text\n",
    "    :return: List of processed unigram, bigram and trigram combinations\n",
    "    \"\"\"\n",
    "    #Convert to lowercase\n",
    "    text = text.lower()\n",
    "    #Remove non-alphabetic characters\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # Word segmentation, stop word filtering, stemming\n",
    "    words = [stemmer.stem(w) for w in text.split() if w not in stop_words]\n",
    "    # Generating unigrams\n",
    "    unigrams = words\n",
    "    # Generating bigram\n",
    "    bigrams = generate_ngrams(words, 2)\n",
    "    #Generating trigram\n",
    "    trigrams = generate_ngrams(words, 3)\n",
    "    # Returns the characteristics of the combination\n",
    "    return unigrams + bigrams + trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d69883b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naive_bayes_trigrams(x_train, y_train):\n",
    "    \"\"\"\n",
    "    Train the Naive Bayes model to count word frequency and category information\n",
    "    :param x_train: training text data\n",
    "    :param y_train: training label data\n",
    "    :return: word frequency dictionary, category count, total number of words in each category, vocabulary\n",
    "    \"\"\"\n",
    "    # Initialize word lists and counts for each category\n",
    "    class_words = {'A': [], 'G': [], 'S': [], 'W': []}\n",
    "    class_counts = {'A': 0, 'G': 0, 'S': 0, 'W': 0}\n",
    "    # Initialize the word frequency dictionary for each category\n",
    "    word_freqs = {'A': {}, 'G': {}, 'S': {}, 'W': {}}\n",
    "    # Initialize vocabulary\n",
    "    vocabulary = set()\n",
    "    #You must not throw the test data in, so we use the divided x_train\n",
    "    for i in range(len(x_train)):\n",
    "        #the data is divided into training set and test set,\n",
    "        # so the index of the data is no longer continuous.\n",
    "        # when looking for the corresponding y, you have to use its own row number, that is, y_train.iloc[i], not y_train[i]\n",
    "        label = y_train.iloc[i]\n",
    "        #Preprocess text to obtain features\n",
    "        words = preprocess_with_trigrams(x_train.iloc[i])\n",
    "        #class counter +1\n",
    "        class_counts[label] += 1\n",
    "        class_words[label].extend(words)\n",
    "        #Update word frequency and vocabulary\n",
    "        #word_freqs[label][word]：Record the number of occurrences (word frequency) of each word under each category (label)\n",
    "        #word_freqs[label]：Access the word frequency dictionary for the current category label (e.g. word_freqs['A'])。\n",
    "        # .get(word, 0)：Try getting the current count for word from the word frequency dictionary:\n",
    "        # If word exists, returns its current value (word frequency). If word does not exist, returns the default value of 0.\n",
    "        # + 1： word_freqs[label][word]。Add 1 to the word frequency (the current word appears once). Store the updated word frequency back to word_freqs[label][word].\n",
    "        for word in words:\n",
    "            word_freqs[label][word] = word_freqs[label].get(word, 0) + 1\n",
    "            vocabulary.add(word)\n",
    "\n",
    "    total_words = {label: len(words) for label, words in class_words.items()}\n",
    "    return word_freqs, class_counts, total_words, list(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "198f0c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_predict_trigrams(test_texts, word_freqs, class_counts, total_words, vocabulary, alpha=0.5):\n",
    "    result = []\n",
    "    labels = ['A', 'G', 'S', 'W']\n",
    "    #Calculate the total number of documents\n",
    "    total_docs = sum(class_counts.values())\n",
    "    # Calculating vocabulary size\n",
    "    vocab_size = len(vocabulary)\n",
    "\n",
    "    for text in test_texts:\n",
    "        #Processing text to obtain features\n",
    "        words = preprocess_with_trigrams(text)\n",
    "        log_probs = {}\n",
    "        #Calculate the log probability for each class\n",
    "        for label in labels:\n",
    "            # Calculate the prior probability (take the logarithm)\n",
    "            prior = math.log(class_counts[label] / total_docs, 2)\n",
    "            likelihood = 0\n",
    "            # Calculate the denominator\n",
    "            denom = total_words[label] + alpha * vocab_size\n",
    "            # Calculate the likelihood probability (take the logarithm)\n",
    "            for word in words:\n",
    "                count = word_freqs[label].get(word, 0)\n",
    "                likelihood += math.log((count + alpha) / denom, 2)\n",
    "            # Store log probability\n",
    "            log_probs[label] = prior + likelihood\n",
    "        # Select the category with the highest probability as the prediction result\n",
    "        predicted_label = max(log_probs, key=log_probs.get)\n",
    "        result.append(predicted_label)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a7e0b807",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_naive_bayes_trigrams(x, y, k=5):\n",
    "    #Initialize cross validation\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    acc_list = []\n",
    "    # Go through each fold\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(x), 1):\n",
    "        x_train, x_val = x.iloc[train_idx], x.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        #Training the model\n",
    "        word_freqs, class_counts, total_words, vocabulary = train_naive_bayes_trigrams(x_train, y_train)\n",
    "        # prediction\n",
    "        y_pred = naive_bayes_predict_trigrams(x_val, word_freqs, class_counts, total_words, vocabulary, alpha=0.5)\n",
    "        # Accuracy\n",
    "        acc = accuracy_score(y_val, y_pred)\n",
    "        acc_list.append(acc)\n",
    "        print(f\"{fold} fold accuracy (Trigrams): {acc:.4f}\")\n",
    "\n",
    "    print(f\"\\n Average accuracy (Trigrams): {np.mean(acc_list):.4f} ± {np.std(acc_list):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "140e0d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy after using Trigrams: 0.9818181818181818\n",
      "\n",
      "Detailed evaluation report after using Trigrams:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.98      0.99      0.99       346\n",
      "           G       0.97      0.91      0.94        43\n",
      "           S       0.81      0.96      0.88        23\n",
      "           W       0.99      0.98      0.99       468\n",
      "\n",
      "    accuracy                           0.98       880\n",
      "   macro avg       0.94      0.96      0.95       880\n",
      "weighted avg       0.98      0.98      0.98       880\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading data and training (using Trigrams functions)\n",
    "data = pd.read_csv(\"train.csv\")\n",
    "x = data[\"Description\"]\n",
    "y = data[\"Class\"]\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "word_freqs, class_counts, total_words, vocabulary = train_naive_bayes_trigrams(x_train, y_train)\n",
    "y_pred = naive_bayes_predict_trigrams(x_test, word_freqs, class_counts, total_words, vocabulary, alpha=0.3)\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\nAccuracy after using Trigrams:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nDetailed evaluation report after using Trigrams:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Run cross validation (using Trigrams function)\n",
    "#cross_validate_naive_bayes_trigrams(x, y, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "44fc94b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on the test set (function using Trigrams)\n",
    "test_set = pd.read_csv(\"test.csv\")\n",
    "test = test_set[\"Description\"]\n",
    "predict = naive_bayes_predict_trigrams(test, word_freqs, class_counts, total_words, vocabulary, alpha=0.3)\n",
    "test_set[\"Class\"] = predict\n",
    "#Save the prediction results to a CSV file\n",
    "test_set.drop([\"Description\"], axis=1).to_csv(\"tst_kaggle_trigrams.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddffcfb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
